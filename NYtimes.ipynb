{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Team:<br> Alexander Umale, <br>Vikrant</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>All the python functions and classes used:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "def getAPIkey(file='./data/nyt_api.key') :\n",
    "    try:\n",
    "        with open(file) as fp:\n",
    "            key = fp.read().strip()\n",
    "            return key\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def searchNYTimes(api_key='', query='', fq='', \n",
    "                   fields='', sort='', begin_date='YYYYMMDD', \n",
    "                   end_date='YYYYMMDD', page=-1,):\n",
    "    api_search_url= 'https://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "\n",
    "    if len(query) < 1:\n",
    "        print('Query string is empty')\n",
    "        return None\n",
    "    \n",
    "    fl_items = ['web_url',\n",
    "    'snippet',\n",
    "    'lead_paragraph',\n",
    "    'abstract',\n",
    "    'print_page',\n",
    "    'blog',\n",
    "    'source',\n",
    "    'multimedia',\n",
    "    'headline',\n",
    "    'keywords',\n",
    "    'pub_date',\n",
    "    'document_type',\n",
    "    'news_desk',\n",
    "    'byline',\n",
    "    'type_of_material',\n",
    "    '_id',\n",
    "    'word_count']\n",
    "    \n",
    "    search_param={'api-key':api_key,\n",
    "                  'q':query }\n",
    "    \n",
    "    if len(fq) > 0 :\n",
    "        search_param['fq'] = fq\n",
    "        \n",
    "    if len(fields) > 0:\n",
    "        if set(fields).issubset(fl_items) :\n",
    "            search_param['fl'] = fields\n",
    "        else:\n",
    "            print('Enter valid field values')\n",
    "            return None\n",
    "    if len(sort) > 0:\n",
    "        if sort == 'newest' | sort == 'oldest':\n",
    "            search_param['sort'] = sort\n",
    "    \n",
    "    if begin_date != 'YYYYMMDD':\n",
    "        if int(begin_date[4:6]) > 0 & int(begin_date[4:6]) <= 12:\n",
    "            if int(begin_date[6:9]) > 0 & int(begin_date[6:9]) <= 31:\n",
    "                search_param['begin_date'] = begin_date\n",
    "                \n",
    "    if end_date != 'YYYYMMDD':\n",
    "        if int(begin_date[4:6]) > 0 & int(begin_date[4:6]) <= 12:\n",
    "            if int(begin_date[6:9]) > 0 & int(begin_date[6:9]) <= 31:\n",
    "                search_param['end_date'] = end_date\n",
    "    \n",
    "    if page >= 0:\n",
    "#        print('page is {}'.format(page))\n",
    "        search_param['page'] = page\n",
    "    \n",
    "    try:\n",
    "#        print('search params: {}'.format(search_param))\n",
    "        resp = requests.get(url=api_search_url,params=search_param)\n",
    "#        print(resp.text)\n",
    "#         print(resp.status_code)\n",
    "        response_json = resp.json()\n",
    "        resp.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    if(response_json != None):\n",
    "        return response_json\n",
    "        \n",
    "\n",
    "class NYTapiResponseWrapper:\n",
    "    def __init__(self, response_json = {}):\n",
    "        if len(response_json.keys()) > 0:\n",
    "            self.status = response_json['status']\n",
    "            self.copyright = response_json['copyright']\n",
    "            self._response = response_json['response']\n",
    "            self._parseResponse(self._response)\n",
    "    \n",
    "    def parseJSON(self, response_json = {} ):\n",
    "        self.status = response_json['status']\n",
    "        self.copyright = response_json['copyright']\n",
    "        self._response = response_json['response']\n",
    "        self._parseResponse(self._response)\n",
    "\n",
    "    def _parseResponse(self, response):\n",
    "        self._docs = response['docs']\n",
    "        self._meta = response['meta']\n",
    "        self._parseDocs(self._docs)\n",
    "    \n",
    "    def _parseDocs(self, docs):\n",
    "        self.docs = []\n",
    "        i = 0\n",
    "        for doc_item in docs:\n",
    "#            print(i)\n",
    "            i += 1\n",
    "            self.docs.append(Doc(doc = doc_item))\n",
    "        \n",
    "        \n",
    "class Doc:\n",
    "    def __init__(self, doc = {}):\n",
    "        self._id = doc['_id']\n",
    "        self.blog             = doc['blog']\n",
    "        self.document_type    = doc['document_type']\n",
    "        self.headline         = doc['headline']\n",
    "        self.keywords         = doc['keywords']\n",
    "        self.multimedia       = doc['multimedia']\n",
    "        self.score            = doc['score']\n",
    "        self.snippet          = doc['snippet']\n",
    "        self.type_of_material = doc['type_of_material']\n",
    "        self.web_url          = doc['web_url']\n",
    "        self.word_count       = doc['word_count']\n",
    "\n",
    "\n",
    "def getPageByURL(URL = ''):\n",
    "    try:\n",
    "        resp = requests.get(url=URL)\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        resp.close()\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def saveArticleText(headline, textParasSoup, filename):\n",
    "    try:\n",
    "        with open(filename, 'w') as fp:\n",
    "            fp.write(headline)\n",
    "            for para in textParasSoup:\n",
    "                fp.write(para.text)\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NYTimes One day Data collection:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 2 article(s) to files\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "api_key = getAPIkey(file = './data/nyt.key')\n",
    "query = 'bitcoin'\n",
    "article_loc = './data/NewsData/OneDay/'\n",
    "\n",
    "\n",
    "article_list = []\n",
    "for page in range(0,101):\n",
    "    resp = searchNYTimes(api_key, query, fq='document_type:article', page=page, begin_date = \"20180404\", end_date=\"20180405\")\n",
    "    resp_ob = NYTapiResponseWrapper(resp)\n",
    "    if len(resp_ob.docs) <= 0 : \n",
    "        break\n",
    "   # print(resp_ob._meta['offset'])\n",
    "    for doc_item in resp_ob.docs:\n",
    "        article = {'id':doc_item._id,'headline':doc_item.headline['main'], 'url':doc_item.web_url, 'downloaded':'N'}\n",
    "        article_list.append(article)\n",
    "    time.sleep(1)\n",
    "    \n",
    "for article in article_list:\n",
    "    article_soup = getPageByURL(URL = article['url']) \n",
    "#     print(article['id'], article['url'])\n",
    "    paras = article_soup.find_all('p', 'story-body-text story-content')\n",
    "    if(len(paras) < 2) :\n",
    "        print(\"Not story-body-text\")\n",
    "        paras = article_soup.find_all('p', 'css-1xyeyil e2kc3sl0')\n",
    "    if(len(paras) < 2):\n",
    "        print(\"Not css-1xyeyil e2kc3sl0\")\n",
    "        paras = article_soup.find_all('p')\n",
    "    with open(article_loc+article['id'], 'w', encoding='utf-8') as file:\n",
    "        file.write(article['headline']+ '\\n')\n",
    "        article_text = ''\n",
    "        for para in paras:\n",
    "            article_text += para.text + '\\n'\n",
    "        file.write(article_text)\n",
    "    article['downloaded']='Y'\n",
    "    time.sleep(1)\n",
    "print(\"wrote \"+ str(len(article_list)) +\" article(s) to files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4>Copy data folder, stopwords.txt and lab2mr.jar to linux VM then execute below shell commands in virtual machine for wordcount mapreduce</h4>\n",
    "<p>hadoop dfs -put ./data/NewsData/OneDay/ NewsDataOneDay\n",
    "<p>hadoop jar ./lab2mr.jar edu.buffalo.mapreduce.WordCount NewsDataOneDay/ NewsWordsOneDay/</p>\n",
    "<p>hadoop dfs -get NewsWordsOneDay/* data/NewsWords/OneDay</p>\n",
    "<p>copy back the output part files to base OS data folder</p>\n",
    "\n",
    "<hr/>\n",
    "<p>Local linux Folder structure:<br/>\n",
    "    .<br/>\n",
    "    ./NewsData/OneDay/<br/>\n",
    "    ./NewsData/OneWeek/<br/>\n",
    "    ./TweetData/OneDay/<br/>\n",
    "    ./TweetData/OneWeek/<br/>\n",
    "    ./lab2mr.jar\n",
    "    \n",
    "</p>\n",
    "\n",
    "<a href='./NYTOneDayVisual.html'>Open visualization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NYTimes One week data collect:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 24 article(s) to files\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "api_key = getAPIkey(file = './data/nyt.key')\n",
    "query = 'bitcoin'\n",
    "article_loc = './data/NewsData/OneWeek/'\n",
    "\n",
    "\n",
    "article_list = []\n",
    "for page in range(0,101):\n",
    "    resp = searchNYTimes(api_key, query, fq='document_type:article', page=page,begin_date = \"20180401\", end_date=\"20180408\")\n",
    "    resp_ob = NYTapiResponseWrapper(resp)\n",
    "    if len(resp_ob.docs) <= 0 : \n",
    "        break\n",
    "    #print(resp_ob._meta['offset'])\n",
    "    for doc_item in resp_ob.docs:\n",
    "        article = {'id':doc_item._id,'headline':doc_item.headline['main'], 'url':doc_item.web_url, 'downloaded':'N'}\n",
    "        article_list.append(article)\n",
    "    time.sleep(1)\n",
    "    \n",
    "for article in article_list:\n",
    "    article_soup = getPageByURL(URL = article['url']) \n",
    "#     print(article['id'], article['url'])\n",
    "    paras = article_soup.find_all('p', 'story-body-text story-content')\n",
    "    if(len(paras) < 2) :\n",
    "#         print(\"Not story-body-text\")\n",
    "        paras = article_soup.find_all('p', 'css-1xyeyil e2kc3sl0')\n",
    "    if(len(paras) < 2):\n",
    "#         print(\"Not css-1xyeyil e2kc3sl0\")\n",
    "        paras = article_soup.find_all('p')\n",
    "    with open(article_loc+article['id'], 'w', encoding='utf-8') as file:\n",
    "        file.write(article['headline']+ '\\n')\n",
    "        article_text = ''\n",
    "        for para in paras:\n",
    "            article_text += para.text + '\\n'\n",
    "        file.write(article_text)\n",
    "    article['downloaded']='Y'\n",
    "    time.sleep(1)\n",
    "print(\"wrote \"+ str(len(article_list)) +\" article(s) to files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4>Copy data/NewsData/OneWeek folder generated to linux VM then execute below shell commands in virtual machine for wordcount mapreduce</h4>\n",
    "<p>hadoop dfs -put ./data/NewsData/OneWeek/ NewsDataOneWeek\n",
    "<p>hadoop jar ./lab2mr.jar edu.buffalo.mapreduce.WordCount NewsDataOneWeek/ NewsWordsOneWeek/</p>\n",
    "<p>hadoop dfs -get NewsWordsOneWeek/* data/NewsWords/OneWeek</p>\n",
    "<p>copy back the output part files to base OS data folder</p>\n",
    "<hr/>\n",
    "\n",
    "<a href='./NYTOneWeekVisual.html'>Open visualization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Co-Occurrence</h2>\n",
    "<h3>Execute below shell commands in virtual machine for Co-occurrence mapreduce: </h3>\n",
    "<p>hadoop jar ./lab2mr.jar edu.buffalo.mapreduce.CoOccurrence NewsDataOneDay/ Cooccur/</p>\n",
    "<p>hadoop dfs -get Cooccur/* data/NewsWords/Cooccur</p>\n",
    "<p>copy back the output part files to base OS data folder</p>\n",
    "<hr/>\n",
    "\n",
    "<a href='./NYTCoocrVisual.html'>Open visualization</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
